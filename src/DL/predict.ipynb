{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __init__ import *\n",
    "from src.utils import config\n",
    "from transformers import BertModel, BertTokenizer, BertConfig, BertForSequenceClassification\n",
    "from src.DL.models.bert import Config, Model\n",
    "import json \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>勇闯 法兰西 此书 的 主人公 罗维孝是 国网 的 一名 退休工人 他 曾 骑车 登上 世界...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>历代 茶 诗集 成宋 金卷 本书 主要 内容 包括 : 丁开 摘句 一首 、 丁带 茶 诗 ...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>蜗牛 作者 用 整整 一部 诗集 在 探索 旧词 新意 的 核心 问题 , 作者 在 后记 ...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>点石成金 雕塑 实验教学 美术 实验教学 丛书 点石成金 : 雕塑 实验教学 的 普及 , ...</td>\n",
       "      <td>艺术</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>文学 原理 新释 这本 文学 原理 新释 在 历经 寒暑 瑞至 岁末 的 时候 终于 脱稿 ...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  category_id\n",
       "0  勇闯 法兰西 此书 的 主人公 罗维孝是 国网 的 一名 退休工人 他 曾 骑车 登上 世界...    文学            0\n",
       "1  历代 茶 诗集 成宋 金卷 本书 主要 内容 包括 : 丁开 摘句 一首 、 丁带 茶 诗 ...    文学            0\n",
       "2  蜗牛 作者 用 整整 一部 诗集 在 探索 旧词 新意 的 核心 问题 , 作者 在 后记 ...    文学            0\n",
       "3  点石成金 雕塑 实验教学 美术 实验教学 丛书 点石成金 : 雕塑 实验教学 的 普及 , ...    艺术            8\n",
       "4  文学 原理 新释 这本 文学 原理 新释 在 历经 寒暑 瑞至 岁末 的 时候 终于 脱稿 ...    文学            0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(config.root_path + '/data/test_clean.tsv', sep='\\t'),\n",
    "                pd.read_csv(config.root_path + '/data/dev_clean.tsv', sep='\\t'),\n",
    "                pd.read_csv(config.root_path + '/data/train_clean.tsv', sep='\\t')]).dropna().reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict(object):\n",
    "    def __init__(self, model_path=config.root_path + '/model/saved_dict/bert.ckpt', \n",
    "                 bert_path=config.root_path + '/model/bert-wwm/',\n",
    "                 is_cuda=config.is_cuda):\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "        self.is_cuda = is_cuda\n",
    "        conf = Config(dataset=config.root_path + '/')\n",
    "        self.model = Model(conf).to(config.device)\n",
    "        checkpoint = torch.load(self.model_path)\n",
    "        self.model.load_state_dict(checkpoint, strict=False)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def process_data(self, text, is_cuda=config.is_cuda):\n",
    "        def padding(indice, max_length, pad_idx=0):\n",
    "            \"\"\"\n",
    "            pad 函数\n",
    "            注意 token type id 右侧pad是添加1而不是0，1表示属于句子B\n",
    "            \"\"\"\n",
    "            pad_indice = [item + [pad_idx] * max(0, max_length - len(item)) for item in indice]\n",
    "            return torch.tensor(pad_indice)\n",
    "        text_dict = self.tokenizer.encode_plus(text,                      # Sentence to encode.\n",
    "                   add_special_tokens=True,   # Add '[CLS]' and '[SEP]'\n",
    "                   max_length=config.max_length,             # Pad & truncate all sentences.\n",
    "                   ad_to_max_length=True,\n",
    "                   return_attention_mask=True,   # Construct attn. masks.\n",
    "#                                                    return_tensors='pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "        input_ids, attention_mask, token_type_ids = text_dict['input_ids'], text_dict['attention_mask'], text_dict['token_type_ids']\n",
    "\n",
    "        token_ids_padded = padding([input_ids], config.max_length)\n",
    "        token_type_ids_padded = padding([token_type_ids], config.max_length)\n",
    "        attention_mask_padded = padding([attention_mask], config.max_length)\n",
    "        return token_ids_padded, token_type_ids_padded, attention_mask_padded\n",
    "        \n",
    "    def predict(self, text):\n",
    "        token_ids_padded, token_type_ids_padded, attention_mask_padded = self.process_data(text)\n",
    "        if self.is_cuda:\n",
    "            token_ids_padded = token_ids_padded.to(torch.device('cuda'))\n",
    "            token_type_ids_padded = token_type_ids_padded.to(torch.device('cuda'))\n",
    "            attention_mask_padded = attention_mask_padded.to(torch.device('cuda'))\n",
    "        outputs = self.model((token_ids_padded, attention_mask_padded, token_type_ids_padded))\n",
    "        label = torch.max(outputs.data, 1)[1].cpu().numpy()[0]\n",
    "        score = outputs.data[0][torch.max(outputs.data, 1)[1].cpu().numpy()[0]].cpu().numpy().tolist()\n",
    "        return label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0511 10:15:32.291856 140495557105408 tokenization_utils.py:420] Model name '/home/user10000254/notespace/textClassification/model/bert-wwm/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/home/user10000254/notespace/textClassification/model/bert-wwm/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0511 10:15:32.303939 140495557105408 tokenization_utils.py:449] Didn't find file /home/user10000254/notespace/textClassification/model/bert-wwm/added_tokens.json. We won't load it.\n",
      "I0511 10:15:32.305000 140495557105408 tokenization_utils.py:449] Didn't find file /home/user10000254/notespace/textClassification/model/bert-wwm/special_tokens_map.json. We won't load it.\n",
      "I0511 10:15:32.306000 140495557105408 tokenization_utils.py:449] Didn't find file /home/user10000254/notespace/textClassification/model/bert-wwm/tokenizer_config.json. We won't load it.\n",
      "I0511 10:15:32.307051 140495557105408 tokenization_utils.py:502] loading file /home/user10000254/notespace/textClassification/model/bert-wwm/vocab.txt\n",
      "I0511 10:15:32.307887 140495557105408 tokenization_utils.py:502] loading file None\n",
      "I0511 10:15:32.308716 140495557105408 tokenization_utils.py:502] loading file None\n",
      "I0511 10:15:32.325023 140495557105408 tokenization_utils.py:502] loading file None\n",
      "I0511 10:15:32.361995 140495557105408 tokenization_utils.py:420] Model name '/home/user10000254/notespace/textClassification/model/bert-wwm/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming '/home/user10000254/notespace/textClassification/model/bert-wwm/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0511 10:15:32.363008 140495557105408 tokenization_utils.py:449] Didn't find file /home/user10000254/notespace/textClassification/model/bert-wwm/added_tokens.json. We won't load it.\n",
      "I0511 10:15:32.363906 140495557105408 tokenization_utils.py:449] Didn't find file /home/user10000254/notespace/textClassification/model/bert-wwm/special_tokens_map.json. We won't load it.\n",
      "I0511 10:15:32.364773 140495557105408 tokenization_utils.py:449] Didn't find file /home/user10000254/notespace/textClassification/model/bert-wwm/tokenizer_config.json. We won't load it.\n",
      "I0511 10:15:32.365595 140495557105408 tokenization_utils.py:502] loading file /home/user10000254/notespace/textClassification/model/bert-wwm/vocab.txt\n",
      "I0511 10:15:32.366246 140495557105408 tokenization_utils.py:502] loading file None\n",
      "I0511 10:15:32.366935 140495557105408 tokenization_utils.py:502] loading file None\n",
      "I0511 10:15:32.367622 140495557105408 tokenization_utils.py:502] loading file None\n",
      "I0511 10:15:32.406493 140495557105408 configuration_utils.py:281] loading configuration file /home/user10000254/notespace/textClassification/model/bert-wwm/config.json\n",
      "I0511 10:15:32.407786 140495557105408 configuration_utils.py:319] Model config BertConfig {\n",
      "  \"_num_labels\": 36,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": null,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"eos_token_id\": null,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0511 10:15:32.412804 140495557105408 modeling_utils.py:505] loading weights file /home/user10000254/notespace/textClassification/model/bert-wwm/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "pred = Predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 29577/294738 [50:45<9:03:19,  8.13it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 14%|█▍        | 41948/294738 [1:12:04<8:27:17,  8.31it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 62%|██████▏   | 183411/294738 [5:24:05<3:18:32,  9.35it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 66%|██████▌   | 194478/294738 [5:44:06<3:24:49,  8.16it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 82%|████████▏ | 240535/294738 [7:05:32<1:33:35,  9.65it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 91%|█████████ | 267570/294738 [7:54:14<57:13,  7.91it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 95%|█████████▌| 280559/294738 [8:16:38<29:32,  8.00it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 98%|█████████▊| 290240/294738 [8:34:36<08:02,  9.32it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "df1 = df['text'].progress_apply(lambda x: pred.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category_id</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>pred_label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>勇闯 法兰西 此书 的 主人公 罗维孝是 国网 的 一名 退休工人 他 曾 骑车 登上 世界...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.291163</td>\n",
       "      <td>文学</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>历代 茶 诗集 成宋 金卷 本书 主要 内容 包括 : 丁开 摘句 一首 、 丁带 茶 诗 ...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.474462</td>\n",
       "      <td>文学</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>蜗牛 作者 用 整整 一部 诗集 在 探索 旧词 新意 的 核心 问题 , 作者 在 后记 ...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.404400</td>\n",
       "      <td>文学</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>点石成金 雕塑 实验教学 美术 实验教学 丛书 点石成金 : 雕塑 实验教学 的 普及 , ...</td>\n",
       "      <td>艺术</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>7.284340</td>\n",
       "      <td>工业技术</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>文学 原理 新释 这本 文学 原理 新释 在 历经 寒暑 瑞至 岁末 的 时候 终于 脱稿 ...</td>\n",
       "      <td>文学</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.938389</td>\n",
       "      <td>文学</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  category_id  \\\n",
       "0  勇闯 法兰西 此书 的 主人公 罗维孝是 国网 的 一名 退休工人 他 曾 骑车 登上 世界...    文学            0   \n",
       "1  历代 茶 诗集 成宋 金卷 本书 主要 内容 包括 : 丁开 摘句 一首 、 丁带 茶 诗 ...    文学            0   \n",
       "2  蜗牛 作者 用 整整 一部 诗集 在 探索 旧词 新意 的 核心 问题 , 作者 在 后记 ...    文学            0   \n",
       "3  点石成金 雕塑 实验教学 美术 实验教学 丛书 点石成金 : 雕塑 实验教学 的 普及 , ...    艺术            8   \n",
       "4  文学 原理 新释 这本 文学 原理 新释 在 历经 寒暑 瑞至 岁末 的 时候 终于 脱稿 ...    文学            0   \n",
       "\n",
       "   pred_label  pred_score pred_label_name  \n",
       "0           0    8.291163              文学  \n",
       "1           0   10.474462              文学  \n",
       "2           0   10.404400              文学  \n",
       "3          24    7.284340            工业技术  \n",
       "4           0    9.938389              文学  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pred_label'] = df1.apply(lambda x: int(x[0]))\n",
    "df['pred_score'] = df1.apply(lambda x: x[1])\n",
    "cat2id = json.load(open(config.root_path + '/data/label2id.json'))\n",
    "id2cat = {v:k for k, v in cat2id.items()}\n",
    "df['pred_label_name'] = df['pred_label'].map(id2cat)\n",
    "# df['label'] = df['label'].map(dct)\n",
    "# df['pred_label_name'] = df['pred_label_name'].map(dct)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['pred_label_name'] != df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 37336 entries, 3 to 294714\n",
      "Data columns (total 6 columns):\n",
      "text               37336 non-null object\n",
      "label              37336 non-null object\n",
      "category_id        37336 non-null int64\n",
      "pred_label         37336 non-null int64\n",
      "pred_score         37336 non-null float64\n",
      "pred_label_name    37336 non-null object\n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df[df['pred_label_name'] != df['label']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8733247833669225\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         少儿     0.9290    0.9669    0.9476     68307\n",
      "         教材     0.8809    0.7923    0.8343      8336\n",
      "         文学     0.6503    0.8422    0.7339      5686\n",
      "       工业技术     0.8556    0.9050    0.8796     31237\n",
      "      中小学教辅     0.9340    0.8800    0.9062      7458\n",
      "         艺术     0.9334    0.8152    0.8703      9113\n",
      "       社会科学     0.6859    0.3603    0.4725      3594\n",
      "         小说     0.8338    0.8554    0.8445     10048\n",
      "      计算机网络     0.8766    0.8711    0.8738     12106\n",
      "         管理     0.8427    0.8005    0.8211      1860\n",
      "         建筑     0.9378    0.9443    0.9410     30154\n",
      "         外语     0.8245    0.7804    0.8018      1571\n",
      "         历史     0.7958    0.8352    0.8150      6937\n",
      "         法律     0.7218    0.6339    0.6750     10471\n",
      "       政治军事     0.9682    0.8990    0.9323      4099\n",
      "       哲学宗教     0.9312    0.8764    0.9030      4524\n",
      "         经济     0.8301    0.8018    0.8157      2346\n",
      "         医学     0.9138    0.8287    0.8692     13002\n",
      "       成功励志     0.7622    0.8147    0.7876      4426\n",
      "       自然科学     0.7465    0.7389    0.7427      6891\n",
      "         考试     0.8995    0.8986    0.8991      5908\n",
      "         传记     0.9142    0.8943    0.9041      9211\n",
      "         文化     0.7532    0.7678    0.7604      5013\n",
      "       青春文学     0.8663    0.8202    0.8426      3933\n",
      "       农业林业     0.8332    0.8739    0.8530     15579\n",
      "       动漫幽默     0.8952    0.8896    0.8924      3369\n",
      "     保健心理健康     0.8981    0.9065    0.9023      1391\n",
      "       家庭教育     0.6246    0.2801    0.3868      1503\n",
      "         美食     0.7519    0.9386    0.8349      1075\n",
      "         古籍     0.6759    0.7363    0.7048      1623\n",
      "       科普读物     0.8522    0.9288    0.8889      1714\n",
      "         旅游     0.8656    0.8992    0.8821      1955\n",
      "      孕产妇育儿     0.7215    0.5738    0.6393       298\n",
      "\n",
      "avg / total     0.8729    0.8733    0.8712    294738\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 33, does not match size of target_names, 36\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[66046,    10,   190, ...,    25,     2,     7],\n",
       "       [   25,  6605,   178, ...,     1,     0,     0],\n",
       "       [   90,    17,  4789, ...,     0,     1,     0],\n",
       "       ...,\n",
       "       [   24,     1,     0, ...,  1592,    18,     1],\n",
       "       [    9,     0,     0, ...,    44,  1758,     1],\n",
       "       [   47,     0,     0, ...,     1,     3,   171]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(df['category_id'], df['pred_label']))\n",
    "print(metrics.classification_report(df['category_id'], df['pred_label'], target_names=[x.strip() for x in open(\n",
    "            '../../data/class.txt').readlines()]   , digits=4))\n",
    "metrics.confusion_matrix(df['category_id'], df['pred_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_label(row):\n",
    "    text = row['text']\n",
    "    pred_score = row['pred_score']\n",
    "    pred_label_name = row['pred_label_name']\n",
    "    label = row['label']\n",
    "    res = label\n",
    "    if ('规划 教材' in text) and (pred_label_name == '大中专教材教辅'):\n",
    "        res = '大中专教材教辅'\n",
    "    elif ('文学' == label) and (pred_label_name == '小说'):\n",
    "        res = '小说'\n",
    "    elif (('高等职业' in text) or ('大学' in text) or ('高职' in text) or \\\n",
    "          ('中等职业' in text) or ('教育部' in text) or ('高等数学' in text) or\\\n",
    "          ('高等院校' in text) or ('教程' in text) or ('教材' in text) or \\\n",
    "          ('高等院校' in text) or (pred_score > 8.))\\\n",
    "        and (pred_label_name == '大中专教材教辅') and (label != '大中专教材教辅'):\n",
    "        res = '大中专教材教辅'\n",
    "    elif ('小说' == label) and (pred_label_name == '文学'):\n",
    "        res = '文学'\n",
    "    elif (pred_score > 8.) and (pred_label_name != label):\n",
    "        res = pred_label_name\n",
    "    elif ('小说' == label) and (pred_label_name == '文学'):\n",
    "        res = '文学'\n",
    "    return res\n",
    "\n",
    "df['label'] = df.apply(lambda row: change_label(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57308, 6)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['pred_label_name'] != df['label']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['pred_score'] > 6.5) & (df['pred_label_name'] == df['label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145734, 6)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230401, 2)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.concat([pd.read_csv(config.root_path + '/data/test_clean.tsv', sep='\\t'),\n",
    "                pd.read_csv(config.root_path + '/data/dev_clean.tsv', sep='\\t'),\n",
    "                pd.read_csv(config.root_path + '/data/train_clean.tsv', sep='\\t')]).reset_index(drop=True)\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302730, 6)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text', 'label', 'category_id']]\n",
    "train, dev, test = np.split(df.sample(frac=1), [int(df.shape[0] * 0.7), int(df.shape[0] * 0.9)])\n",
    "train.to_csv('../../data/train_clean.tsv', sep='\\t', index=False)\n",
    "dev.to_csv('../../data/dev_clean.tsv', sep='\\t', index=False)\n",
    "test.to_csv('../../data/test_clean.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "df['category_id'] = df['label'].factorize()[0]\n",
    "category_id_df = df[['label', 'category_id']].drop_duplicates()\n",
    "with open('../../data/label2id.json', 'w') as f:\n",
    "    json.dump({k: v for k, v in zip(category_id_df['label'], category_id_df['category_id'])}, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['label'].isin(['传记', '经济', '两性关系', '社会科学', '孕产妇育儿', '家庭教育'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "文学         68307\n",
       "大中专教材教辅    31237\n",
       "童书         30154\n",
       "工业技术       15579\n",
       "中小学教辅      13002\n",
       "艺术         12106\n",
       "社会科学       10471\n",
       "小说         10048\n",
       "计算机与互联网     9211\n",
       "建筑          9113\n",
       "管理          8336\n",
       "外语学习        7458\n",
       "科学与自然       6937\n",
       "历史          6891\n",
       "法律          5908\n",
       "政治/军事       5686\n",
       "哲学/宗教       5013\n",
       "医学          4524\n",
       "励志与成功       4426\n",
       "考试          4099\n",
       "青春文学        3933\n",
       "文化          3594\n",
       "农业/林业       3369\n",
       "动漫          2346\n",
       "健身与保健       1955\n",
       "育儿/家教       1860\n",
       "烹饪/美食       1714\n",
       "国学/古籍       1623\n",
       "旅游/地图       1571\n",
       "科普读物        1503\n",
       "孕产/胎教       1391\n",
       "金融与投资       1075\n",
       "婚恋与两性        298\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
